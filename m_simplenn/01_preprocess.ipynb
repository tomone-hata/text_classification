{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なモジュールのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "import copy\n",
    "from tqdm import tqdm_notebook\n",
    "from utils import label_to_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習/テストデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train_data.pickle', 'rb') as rb:\n",
    "    train_data = pickle.load(rb)\n",
    "\n",
    "with open('../data/test_data.pickle', 'rb') as rb:\n",
    "    test_data = pickle.load(rb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_data, columns=['label', 'text'])\n",
    "df_test = pd.DataFrame(test_data, columns=['label', 'text'])\n",
    "\n",
    "print('train_data')\n",
    "display(df_train.head())\n",
    "display(df_train['label'].value_counts())\n",
    "print()\n",
    "print('test_data')\n",
    "display(df_test.head())\n",
    "display(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ラベルとテキストに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text = [x[1] for x in train_data]\n",
    "y_train = [x[0] for x in train_data]\n",
    "x_test_text = [x[1] for x in test_data]\n",
    "y_test = [x[0] for x in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストをベクトルに変換する\n",
    "今回は、word2vecモデルを使用してテキストを構成する単語をベクトルに変換し、\n",
    "その平均値をとる。  \n",
    "学習データ(orテストデータ)のi番目のラベルの文書全体を$ \\textit{D}_{i} $とし、$ \\textit{D}_{i} $が$ k $個の単語で構成されているとすると\n",
    "$$\n",
    "\\textit{D}_{i} : {x_{1}, x_{2},... x_{k}} \n",
    "$$\n",
    "となるので、$x_{j} \\in D_{i}$をword2vecに変換したベクトルを$\\boldsymbol{x}_{j}$とすると$ \\textit{D}_{i} $の文書ベクトルを\n",
    "$$\n",
    "\\frac{1}{k} \\sum^{k}_{i=1} \\boldsymbol{x}_{i}\n",
    "$$\n",
    "とする。$\\boldsymbol{x}_{j}$が存在しない単語については除外する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = word2vec.Word2Vec.load(\"yagi/word2vec.gensim.model\")\n",
    "model = word2vec.Word2Vec.load('../data/basic_words_model.model')\n",
    "def text_to_vector(text_list):\n",
    "    text_vectors = []\n",
    "    for text in text_list:\n",
    "        words = re.split(r'( +)|\\n', text)\n",
    "        words = [x for x in words if x]\n",
    "        for word in words:\n",
    "            try:\n",
    "                if word :\n",
    "                    vector = model[word]\n",
    "                    text_vectors.append(copy.copy(vector))\n",
    "            except KeyError as f:\n",
    "                pass\n",
    "    text_vectors = np.array(text_vectors)\n",
    "    return np.average(text_vectors, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train = np.array([text_to_vector(x) for x in tqdm_notebook(x_train_text)])\n",
    "x_test = np.array([text_to_vector(x) for x in tqdm_notebook(x_test_text)])\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラベルをIDに置き換える\n",
    "今回分析対象の委員会を以下の通り通し番号をつける。\n",
    "\n",
    "|委員会 |番号|\n",
    "|---|---|\n",
    "|本会議|0|\n",
    "|内閣委員会|1|\n",
    "|総務委員会|2|\n",
    "|法務委員会|3|\n",
    "|外務委員会|4|\n",
    "|財務金融委員会|5|\n",
    "|文部科学委員会|6|\n",
    "|厚生労働委員会|7|\n",
    "|農林水産委員会|8|\n",
    "|経済産業委員会|9|\n",
    "|国土交通委員会|10|\n",
    "|環境委員会|11|\n",
    "|安全保障委員会|12|\n",
    "|予算委員会|13|\n",
    "|決算行政監視委員会|14|\n",
    "|議院運営委員会|15|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classify_dict = {'本会議':0, '内閣委員会':1, '総務委員会':2, '法務委員会':3, '外務委員会':4, '財務金融委員会':5, '文部科学委員会':6, '厚生労働委員会':7, '農林水産委員会':8, \\\n",
    "                '経済産業委員会':9, '国土交通委員会':10, '環境委員会':11, '安全保障委員会':12, '予算委員会':13, '決算行政監視委員会':14, '議院運営委員会':15}\n",
    "classify_size = len(classify_dict)\n",
    "y_train = np.array([classify_dict[x] for x in tqdm_notebook(y_train)])\n",
    "y_test = np.array([classify_dict[x] for x in tqdm_notebook(y_test)])\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラスの重みを算出する\n",
    "分析対象の委員会のデータ数のばらつきが大きいため、少ないクラス(委員会)の学習時のコストを大きくする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.arange(16)\n",
    "cost_weight = np.zeros((16))\n",
    "for label in labels:\n",
    "    cnt = np.sum(y_train==label)\n",
    "    cost_weight[label] = cnt\n",
    "\n",
    "max_cnt = np.max(cost_weight)\n",
    "cost_weight= np.reciprocal(cost_weight) * max_cnt\n",
    "cost_weight = {int(label):cost_weight[label] for label in labels}\n",
    "cost_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hotベクトルの作成\n",
    "上記通し番号に沿って、one-hotベクトルに変換する。以下に法務委員会の場合のone-hotベクトルを示す。\n",
    "$$\n",
    "\\left(\\begin{array}{c}\n",
    "            0 \\\\\n",
    "            0 \\\\\n",
    "            0 \\\\\n",
    "            1 \\\\\n",
    "            0 \\\\\n",
    "            ... \\\\\n",
    "            0 \\\\\n",
    "        \\end{array}\\right) \\quad\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_size = len(classify_dict)\n",
    "y_train = np.array([label_to_one_hot(classify_size, x) for x in tqdm_notebook(y_train)])\n",
    "y_test = np.array([label_to_one_hot(classify_size ,x) for x in tqdm_notebook(y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data.pickle', 'wb') as wb:\n",
    "    pickle.dump((x_train, y_train), wb)\n",
    "    \n",
    "with open('test_data.pickle', 'wb') as wb:\n",
    "    pickle.dump((x_test, y_test), wb)\n",
    "    \n",
    "with open('cost_weight.pickle', 'wb') as wb:\n",
    "    pickle.dump(cost_weight, wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
